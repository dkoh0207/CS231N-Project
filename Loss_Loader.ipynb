{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.14/04\n"
     ]
    }
   ],
   "source": [
    "# Import Dependencies\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "#%matplotlib notebook\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../new_notebooks/ipynb/dlp_opendata_api\")\n",
    "from osf.image_api import image_reader_3d\n",
    "from osf.particle_api import *\n",
    "from osf.cluster_api import *\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import sparseconvnet as scn\n",
    "import glob\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusteringAEData(Dataset):\n",
    "    \"\"\"\n",
    "    A customized data loader for clustering.\n",
    "    \"\"\"\n",
    "    def __init__(self, root, numPixels=192, filenames=None):\n",
    "        \"\"\"\n",
    "        Initialize Clustering Dataset\n",
    "\n",
    "        Inputs:\n",
    "            - root: root directory of dataset\n",
    "            - preload: if preload dataset into memory.\n",
    "        \"\"\"\n",
    "        self.cluster_filenames = []\n",
    "        self.energy_filenames = []\n",
    "        self.root = root\n",
    "        self.numPixels = str(numPixels)\n",
    "        \n",
    "        if filenames:\n",
    "            self.energy_filenames = filenames[0]\n",
    "            self.cluster_filenames = filenames[1]\n",
    "            print(self.energy_filenames)\n",
    "\n",
    "        self.energy_filenames.sort()\n",
    "        self.cluster_filenames.sort()\n",
    "        self.cluster_reader = cluster_reader(*self.cluster_filenames)\n",
    "        self.energy_reader = image_reader_3d(*self.energy_filenames)\n",
    "        self.len = self.energy_reader.entry_count()\n",
    "        assert self.len == self.cluster_reader.entry_count()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get a sample from dataset.\n",
    "        \"\"\"\n",
    "        voxel, label = self.cluster_reader.get_image(index)\n",
    "        _, energy, _ = self.energy_reader.get_image(index)\n",
    "        voxel, label = torch.from_numpy(voxel), torch.from_numpy(label)\n",
    "        energy = torch.from_numpy(energy)\n",
    "        energy = torch.unsqueeze(energy, dim=1)\n",
    "        label = torch.unsqueeze(label, dim=1).type(torch.LongTensor)\n",
    "        return (voxel, energy), label\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Total number of sampels in dataset.\n",
    "        \"\"\"\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ae_collate(batch):\n",
    "    \"\"\"\n",
    "    Custom collate_fn for Autoencoder.\n",
    "    \"\"\"\n",
    "    data = [item[0] for item in batch]\n",
    "    target = [item[1] for item in batch]\n",
    "    return [data, target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/cluster/dlprod_cluster_192px_00.root\n",
      "/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/dlprod_192px_00.root\n",
      "/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/cluster/dlprod_cluster_192px_01.root\n",
      "/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/dlprod_192px_01.root\n",
      "/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/cluster/dlprod_cluster_192px_02.root\n",
      "/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/dlprod_192px_02.root\n",
      "/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/cluster/dlprod_cluster_192px_03.root\n",
      "/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/dlprod_192px_03.root\n",
      "/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/cluster/dlprod_cluster_192px_04.root\n",
      "/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/dlprod_192px_04.root\n",
      "/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/cluster/dlprod_cluster_192px_05.root\n",
      "/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/dlprod_192px_05.root\n",
      "/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/cluster/dlprod_cluster_192px_06.root\n",
      "/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/dlprod_192px_06.root\n",
      "/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/cluster/dlprod_cluster_192px_07.root\n",
      "/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/dlprod_192px_07.root\n",
      "/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/cluster/dlprod_cluster_192px_08.root\n",
      "/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/dlprod_192px_08.root\n",
      "['/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/dlprod_192px_00.root', '/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/dlprod_192px_01.root', '/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/dlprod_192px_02.root', '/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/dlprod_192px_03.root', '/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/dlprod_192px_04.root', '/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/dlprod_192px_05.root', '/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/dlprod_192px_06.root', '/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/dlprod_192px_07.root']\n",
      "['/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/dlprod_192px_08.root']\n",
      "Number of entries in training set: 80000\n",
      "Number of entries in validation set: 10000\n"
     ]
    }
   ],
   "source": [
    "root = '/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10' #replace with your own path to root folder. \n",
    "trainset_cluster = [root + '/cluster/dlprod_cluster_192px_0{}.root'.format(i) for i in range(8)]\n",
    "devset_cluster = [root + '/cluster/dlprod_cluster_192px_0{}.root'.format(8)]\n",
    "#testset_cluster = [root + '/cluster/dlprod_cluster_192px_0{}.root'.format(9)]\n",
    "\n",
    "trainset_energy = [root + '/dlprod_192px_0{}.root'.format(i) for i in range(8)]\n",
    "devset_energy = [root + '/dlprod_192px_0{}.root'.format(8)]\n",
    "#testset_energy = [root + '/dlprod_192px_0{}.root'.format(9)]\n",
    "\n",
    "for i, f in enumerate(trainset_cluster):\n",
    "    print(f)\n",
    "    print(trainset_energy[i])\n",
    "    \n",
    "for i, f in enumerate(devset_cluster):\n",
    "    print(f)\n",
    "    print(devset_energy[i])\n",
    "    \n",
    "#for i, f in enumerate(testset_cluster):\n",
    "#    print(f)\n",
    "#    print(testset_energy[i])\n",
    "\n",
    "trainset = ClusteringAEData(root, 192, filenames=[trainset_energy, trainset_cluster])\n",
    "devset = ClusteringAEData(root, 192, filenames=[devset_energy, devset_cluster])\n",
    "#testset = ClusteringAEData(root, 192, filenames=[testset_energy, testset_cluster])\n",
    "print('Number of entries in training set: {}'.format(len(trainset)))\n",
    "print('Number of entries in validation set: {}'.format(len(devset)))\n",
    "#print('Number of entries in test set: {}'.format(len(testset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset, batch_size=16, shuffle=True, collate_fn=ae_collate, num_workers=1, pin_memory=False)\n",
    "devloader = DataLoader(devset, batch_size=16, shuffle=True, collate_fn=ae_collate, num_workers=1, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UResNet(torch.nn.Module):\n",
    "    def __init__(self, dim=3, size=192, nFeatures=16, depth=5, nClasses=5):\n",
    "        import sparseconvnet as scn\n",
    "        super(UResNet, self).__init__()\n",
    "        #self._flags = flags\n",
    "        dimension = dim\n",
    "        reps = 2  # Conv block repetition factor\n",
    "        kernel_size = 2  # Use input_spatial_size method for other values?\n",
    "        m = nFeatures  # Unet number of features\n",
    "        nPlanes = [i*m for i in range(1, depth+1)]  # UNet number of features per level\n",
    "        # nPlanes = [(2**i) * m for i in range(1, num_strides+1)]  # UNet number of features per level\n",
    "        nInputFeatures = 1\n",
    "        self.sparseModel = scn.Sequential().add(\n",
    "           scn.InputLayer(dimension, size, mode=3)).add(\n",
    "           scn.SubmanifoldConvolution(dimension, nInputFeatures, m, 3, False)).add( # Kernel size 3, no bias\n",
    "           scn.UNet(dimension, reps, nPlanes, residual_blocks=True, downsample=[kernel_size, 2])).add(  # downsample = [filter size, filter stride]\n",
    "           scn.BatchNormReLU(m)).add(\n",
    "           scn.OutputLayer(dimension))\n",
    "        self.linear = torch.nn.Linear(m, nClasses)\n",
    "\n",
    "    def forward(self, point_cloud):\n",
    "        \"\"\"\n",
    "        point_cloud is a list of length minibatch size (assumes mbs = 1)\n",
    "        point_cloud[0] has 3 spatial coordinates + 1 batch coordinate + 1 feature\n",
    "        shape of point_cloud[0] = (N, 4)\n",
    "        \"\"\"\n",
    "        coords = point_cloud[:, 0:-1].float()\n",
    "        features = point_cloud[:, -1][:, None].float()\n",
    "        x = self.sparseModel((coords, features))\n",
    "        x = self.linear(x)\n",
    "        return [x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unet(fname, dimension=3, size=192, nFeatures=16, depth=5, nClasses=5):\n",
    "    model = UResNet(dim=dimension, size=size, nFeatures=nFeatures, depth=depth, nClasses=nClasses)\n",
    "    model = nn.DataParallel(model)\n",
    "    #print(model.state_dict().keys())\n",
    "    checkpoint = torch.load(fname, map_location='cpu')\n",
    "    #print()\n",
    "    #print(checkpoint['state_dict'].keys())\n",
    "    model.load_state_dict(checkpoint['state_dict'], strict=True)\n",
    "    # just return the pre-trained unet\n",
    "    return model.module.sparseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): InputLayer()\n",
       "  (1): SubmanifoldConvolution 1->16 C3\n",
       "  (2): Sequential(\n",
       "    (0): ConcatTable(\n",
       "      (0): Identity()\n",
       "      (1): Sequential(\n",
       "        (0): BatchNormReLU(16,eps=0.0001,momentum=0.9,affine=True)\n",
       "        (1): SubmanifoldConvolution 16->16 C3\n",
       "        (2): BatchNormReLU(16,eps=0.0001,momentum=0.9,affine=True)\n",
       "        (3): SubmanifoldConvolution 16->16 C3\n",
       "      )\n",
       "    )\n",
       "    (1): AddTable()\n",
       "    (2): ConcatTable(\n",
       "      (0): Identity()\n",
       "      (1): Sequential(\n",
       "        (0): BatchNormReLU(16,eps=0.0001,momentum=0.9,affine=True)\n",
       "        (1): SubmanifoldConvolution 16->16 C3\n",
       "        (2): BatchNormReLU(16,eps=0.0001,momentum=0.9,affine=True)\n",
       "        (3): SubmanifoldConvolution 16->16 C3\n",
       "      )\n",
       "    )\n",
       "    (3): AddTable()\n",
       "    (4): ConcatTable(\n",
       "      (0): Identity()\n",
       "      (1): Sequential(\n",
       "        (0): BatchNormReLU(16,eps=0.0001,momentum=0.9,affine=True)\n",
       "        (1): Convolution 16->32 C2/2\n",
       "        (2): Sequential(\n",
       "          (0): ConcatTable(\n",
       "            (0): Identity()\n",
       "            (1): Sequential(\n",
       "              (0): BatchNormReLU(32,eps=0.0001,momentum=0.9,affine=True)\n",
       "              (1): SubmanifoldConvolution 32->32 C3\n",
       "              (2): BatchNormReLU(32,eps=0.0001,momentum=0.9,affine=True)\n",
       "              (3): SubmanifoldConvolution 32->32 C3\n",
       "            )\n",
       "          )\n",
       "          (1): AddTable()\n",
       "          (2): ConcatTable(\n",
       "            (0): Identity()\n",
       "            (1): Sequential(\n",
       "              (0): BatchNormReLU(32,eps=0.0001,momentum=0.9,affine=True)\n",
       "              (1): SubmanifoldConvolution 32->32 C3\n",
       "              (2): BatchNormReLU(32,eps=0.0001,momentum=0.9,affine=True)\n",
       "              (3): SubmanifoldConvolution 32->32 C3\n",
       "            )\n",
       "          )\n",
       "          (3): AddTable()\n",
       "          (4): ConcatTable(\n",
       "            (0): Identity()\n",
       "            (1): Sequential(\n",
       "              (0): BatchNormReLU(32,eps=0.0001,momentum=0.9,affine=True)\n",
       "              (1): Convolution 32->48 C2/2\n",
       "              (2): Sequential(\n",
       "                (0): ConcatTable(\n",
       "                  (0): Identity()\n",
       "                  (1): Sequential(\n",
       "                    (0): BatchNormReLU(48,eps=0.0001,momentum=0.9,affine=True)\n",
       "                    (1): SubmanifoldConvolution 48->48 C3\n",
       "                    (2): BatchNormReLU(48,eps=0.0001,momentum=0.9,affine=True)\n",
       "                    (3): SubmanifoldConvolution 48->48 C3\n",
       "                  )\n",
       "                )\n",
       "                (1): AddTable()\n",
       "                (2): ConcatTable(\n",
       "                  (0): Identity()\n",
       "                  (1): Sequential(\n",
       "                    (0): BatchNormReLU(48,eps=0.0001,momentum=0.9,affine=True)\n",
       "                    (1): SubmanifoldConvolution 48->48 C3\n",
       "                    (2): BatchNormReLU(48,eps=0.0001,momentum=0.9,affine=True)\n",
       "                    (3): SubmanifoldConvolution 48->48 C3\n",
       "                  )\n",
       "                )\n",
       "                (3): AddTable()\n",
       "                (4): ConcatTable(\n",
       "                  (0): Identity()\n",
       "                  (1): Sequential(\n",
       "                    (0): BatchNormReLU(48,eps=0.0001,momentum=0.9,affine=True)\n",
       "                    (1): Convolution 48->64 C2/2\n",
       "                    (2): Sequential(\n",
       "                      (0): ConcatTable(\n",
       "                        (0): Identity()\n",
       "                        (1): Sequential(\n",
       "                          (0): BatchNormReLU(64,eps=0.0001,momentum=0.9,affine=True)\n",
       "                          (1): SubmanifoldConvolution 64->64 C3\n",
       "                          (2): BatchNormReLU(64,eps=0.0001,momentum=0.9,affine=True)\n",
       "                          (3): SubmanifoldConvolution 64->64 C3\n",
       "                        )\n",
       "                      )\n",
       "                      (1): AddTable()\n",
       "                      (2): ConcatTable(\n",
       "                        (0): Identity()\n",
       "                        (1): Sequential(\n",
       "                          (0): BatchNormReLU(64,eps=0.0001,momentum=0.9,affine=True)\n",
       "                          (1): SubmanifoldConvolution 64->64 C3\n",
       "                          (2): BatchNormReLU(64,eps=0.0001,momentum=0.9,affine=True)\n",
       "                          (3): SubmanifoldConvolution 64->64 C3\n",
       "                        )\n",
       "                      )\n",
       "                      (3): AddTable()\n",
       "                      (4): ConcatTable(\n",
       "                        (0): Identity()\n",
       "                        (1): Sequential(\n",
       "                          (0): BatchNormReLU(64,eps=0.0001,momentum=0.9,affine=True)\n",
       "                          (1): Convolution 64->80 C2/2\n",
       "                          (2): Sequential(\n",
       "                            (0): ConcatTable(\n",
       "                              (0): Identity()\n",
       "                              (1): Sequential(\n",
       "                                (0): BatchNormReLU(80,eps=0.0001,momentum=0.9,affine=True)\n",
       "                                (1): SubmanifoldConvolution 80->80 C3\n",
       "                                (2): BatchNormReLU(80,eps=0.0001,momentum=0.9,affine=True)\n",
       "                                (3): SubmanifoldConvolution 80->80 C3\n",
       "                              )\n",
       "                            )\n",
       "                            (1): AddTable()\n",
       "                            (2): ConcatTable(\n",
       "                              (0): Identity()\n",
       "                              (1): Sequential(\n",
       "                                (0): BatchNormReLU(80,eps=0.0001,momentum=0.9,affine=True)\n",
       "                                (1): SubmanifoldConvolution 80->80 C3\n",
       "                                (2): BatchNormReLU(80,eps=0.0001,momentum=0.9,affine=True)\n",
       "                                (3): SubmanifoldConvolution 80->80 C3\n",
       "                              )\n",
       "                            )\n",
       "                            (3): AddTable()\n",
       "                          )\n",
       "                          (3): BatchNormReLU(80,eps=0.0001,momentum=0.9,affine=True)\n",
       "                          (4): Deconvolution 80->64 C2/2\n",
       "                        )\n",
       "                      )\n",
       "                      (5): JoinTable()\n",
       "                      (6): ConcatTable(\n",
       "                        (0): NetworkInNetwork128->64\n",
       "                        (1): Sequential(\n",
       "                          (0): BatchNormReLU(128,eps=0.0001,momentum=0.9,affine=True)\n",
       "                          (1): SubmanifoldConvolution 128->64 C3\n",
       "                          (2): BatchNormReLU(64,eps=0.0001,momentum=0.9,affine=True)\n",
       "                          (3): SubmanifoldConvolution 64->64 C3\n",
       "                        )\n",
       "                      )\n",
       "                      (7): AddTable()\n",
       "                      (8): ConcatTable(\n",
       "                        (0): Identity()\n",
       "                        (1): Sequential(\n",
       "                          (0): BatchNormReLU(64,eps=0.0001,momentum=0.9,affine=True)\n",
       "                          (1): SubmanifoldConvolution 64->64 C3\n",
       "                          (2): BatchNormReLU(64,eps=0.0001,momentum=0.9,affine=True)\n",
       "                          (3): SubmanifoldConvolution 64->64 C3\n",
       "                        )\n",
       "                      )\n",
       "                      (9): AddTable()\n",
       "                    )\n",
       "                    (3): BatchNormReLU(64,eps=0.0001,momentum=0.9,affine=True)\n",
       "                    (4): Deconvolution 64->48 C2/2\n",
       "                  )\n",
       "                )\n",
       "                (5): JoinTable()\n",
       "                (6): ConcatTable(\n",
       "                  (0): NetworkInNetwork96->48\n",
       "                  (1): Sequential(\n",
       "                    (0): BatchNormReLU(96,eps=0.0001,momentum=0.9,affine=True)\n",
       "                    (1): SubmanifoldConvolution 96->48 C3\n",
       "                    (2): BatchNormReLU(48,eps=0.0001,momentum=0.9,affine=True)\n",
       "                    (3): SubmanifoldConvolution 48->48 C3\n",
       "                  )\n",
       "                )\n",
       "                (7): AddTable()\n",
       "                (8): ConcatTable(\n",
       "                  (0): Identity()\n",
       "                  (1): Sequential(\n",
       "                    (0): BatchNormReLU(48,eps=0.0001,momentum=0.9,affine=True)\n",
       "                    (1): SubmanifoldConvolution 48->48 C3\n",
       "                    (2): BatchNormReLU(48,eps=0.0001,momentum=0.9,affine=True)\n",
       "                    (3): SubmanifoldConvolution 48->48 C3\n",
       "                  )\n",
       "                )\n",
       "                (9): AddTable()\n",
       "              )\n",
       "              (3): BatchNormReLU(48,eps=0.0001,momentum=0.9,affine=True)\n",
       "              (4): Deconvolution 48->32 C2/2\n",
       "            )\n",
       "          )\n",
       "          (5): JoinTable()\n",
       "          (6): ConcatTable(\n",
       "            (0): NetworkInNetwork64->32\n",
       "            (1): Sequential(\n",
       "              (0): BatchNormReLU(64,eps=0.0001,momentum=0.9,affine=True)\n",
       "              (1): SubmanifoldConvolution 64->32 C3\n",
       "              (2): BatchNormReLU(32,eps=0.0001,momentum=0.9,affine=True)\n",
       "              (3): SubmanifoldConvolution 32->32 C3\n",
       "            )\n",
       "          )\n",
       "          (7): AddTable()\n",
       "          (8): ConcatTable(\n",
       "            (0): Identity()\n",
       "            (1): Sequential(\n",
       "              (0): BatchNormReLU(32,eps=0.0001,momentum=0.9,affine=True)\n",
       "              (1): SubmanifoldConvolution 32->32 C3\n",
       "              (2): BatchNormReLU(32,eps=0.0001,momentum=0.9,affine=True)\n",
       "              (3): SubmanifoldConvolution 32->32 C3\n",
       "            )\n",
       "          )\n",
       "          (9): AddTable()\n",
       "        )\n",
       "        (3): BatchNormReLU(32,eps=0.0001,momentum=0.9,affine=True)\n",
       "        (4): Deconvolution 32->16 C2/2\n",
       "      )\n",
       "    )\n",
       "    (5): JoinTable()\n",
       "    (6): ConcatTable(\n",
       "      (0): NetworkInNetwork32->16\n",
       "      (1): Sequential(\n",
       "        (0): BatchNormReLU(32,eps=0.0001,momentum=0.9,affine=True)\n",
       "        (1): SubmanifoldConvolution 32->16 C3\n",
       "        (2): BatchNormReLU(16,eps=0.0001,momentum=0.9,affine=True)\n",
       "        (3): SubmanifoldConvolution 16->16 C3\n",
       "      )\n",
       "    )\n",
       "    (7): AddTable()\n",
       "    (8): ConcatTable(\n",
       "      (0): Identity()\n",
       "      (1): Sequential(\n",
       "        (0): BatchNormReLU(16,eps=0.0001,momentum=0.9,affine=True)\n",
       "        (1): SubmanifoldConvolution 16->16 C3\n",
       "        (2): BatchNormReLU(16,eps=0.0001,momentum=0.9,affine=True)\n",
       "        (3): SubmanifoldConvolution 16->16 C3\n",
       "      )\n",
       "    )\n",
       "    (9): AddTable()\n",
       "  )\n",
       "  (3): BatchNormReLU(16,eps=0.0001,momentum=0.9,affine=True)\n",
       "  (4): OutputLayer()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname = '/gpfs/slac/staas/fs1/g/neutrino/.scn_paper/new/sparse_is192_uns5_uf16_bs64/weights3/snapshot-29999.ckpt'\n",
    "unet = get_unet(fname)\n",
    "unet.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainiter = iter(trainloader)\n",
    "data, labels = trainiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size = 16\n",
      "labels.shape = torch.Size([4860, 1])\n",
      "coords.shape = torch.Size([4860, 3])\n",
      "energy.shape = torch.Size([4860, 1])\n",
      "--------------------\n",
      "labels.shape = torch.Size([2676, 1])\n",
      "coords.shape = torch.Size([2676, 3])\n",
      "energy.shape = torch.Size([2676, 1])\n",
      "--------------------\n",
      "labels.shape = torch.Size([2096, 1])\n",
      "coords.shape = torch.Size([2096, 3])\n",
      "energy.shape = torch.Size([2096, 1])\n",
      "--------------------\n",
      "labels.shape = torch.Size([2516, 1])\n",
      "coords.shape = torch.Size([2516, 3])\n",
      "energy.shape = torch.Size([2516, 1])\n",
      "--------------------\n",
      "labels.shape = torch.Size([3247, 1])\n",
      "coords.shape = torch.Size([3247, 3])\n",
      "energy.shape = torch.Size([3247, 1])\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Batch Size = {}\".format(len(data)))\n",
    "for i in range(min(len(data), 5)):\n",
    "    print(\"labels.shape = {}\".format(labels[i].shape))\n",
    "    print(\"coords.shape = {}\".format(data[i][0].shape))\n",
    "    print(\"energy.shape = {}\".format(data[i][1].shape))\n",
    "    print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord, energy = data[0]\n",
    "label = labels[0]\n",
    "coord, energy = coord.cuda(), energy.cuda()\n",
    "out = unet((coord, energy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Clusters = torch.Size([12])\n",
      "[0, 3, 4, 5, 6, 7, 8, 9, 10, 14, 24, 48]\n",
      "tensor([ 0,  3,  4,  5,  6,  7,  8,  9, 10, 14, 24, 48])\n"
     ]
    }
   ],
   "source": [
    "n_cluster = label.unique().size()\n",
    "print(\"Number of Clusters = {}\".format(n_cluster))\n",
    "cluster_labels = label.unique(sorted=True)\n",
    "print(list(cluster_labels.numpy()))\n",
    "print(cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Mean = tensor([0.0392, 0.0302, 0.0005, 0.0697, 0.5309, 0.0738, 0.0050, 0.0164, 6.2792,\n",
      "        0.1473, 1.2774, 4.8091, 1.1092, 0.0260, 0.2112, 5.2653],\n",
      "       grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Compute average of cluster\n",
    "#(label == 0).nonzero().squeeze(1)\n",
    "index = (label == 14).squeeze(1).nonzero()\n",
    "index = index.squeeze(1)\n",
    "mu_c = out[index].mean(0)\n",
    "print(\"Cluster Mean = {}\".format(mu_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for computing centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cluster_means(features, label):\n",
    "    '''\n",
    "    For a given image, compute the mean clustering point mu_c for each\n",
    "    cluster label in the feature dimension.\n",
    "    '''\n",
    "    n_clusters = label.unique().size()\n",
    "    cluster_labels = list(label.unique(sorted=True).numpy())\n",
    "    # Ordering of the cluster means are crucial.\n",
    "    cluster_means = []\n",
    "    for c in cluster_labels:\n",
    "        index = (label == c).squeeze(1).nonzero()\n",
    "        index = index.squeeze(1)\n",
    "        mu_c = features[index].mean(0)\n",
    "        cluster_means.append(mu_c)\n",
    "    cluster_means = torch.stack(cluster_means)\n",
    "    return cluster_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.4266, 2.6478, 1.8963, 0.0000, 0.2725, 0.0150, 0.0118, 0.4292, 0.4624,\n",
      "         0.0122, 1.6712, 0.0000, 2.5536, 0.0003, 2.2377, 2.9073],\n",
      "        [2.0479, 2.0550, 1.1468, 0.0000, 0.3504, 0.0278, 0.0098, 0.3071, 1.1611,\n",
      "         0.0341, 1.7235, 0.0450, 2.3116, 0.0007, 2.0758, 3.0324],\n",
      "        [1.9780, 0.0322, 1.4784, 0.0000, 0.0001, 0.0200, 2.4804, 0.0051, 1.6261,\n",
      "         1.8203, 0.0006, 0.0010, 0.3292, 0.8389, 3.1172, 0.0036],\n",
      "        [1.8207, 0.9258, 1.3938, 0.0000, 0.3216, 0.0939, 0.0515, 0.9003, 0.2954,\n",
      "         0.1475, 0.6243, 0.0000, 1.7631, 0.0030, 2.1722, 2.0616],\n",
      "        [2.3648, 0.0000, 0.1677, 0.0000, 0.0000, 0.0230, 0.2310, 0.1347, 0.2688,\n",
      "         0.4499, 0.0000, 0.0000, 1.0779, 0.0000, 2.6447, 0.0225],\n",
      "        [2.5145, 0.0274, 0.1270, 0.0000, 0.0000, 0.0255, 0.1967, 0.2036, 0.2963,\n",
      "         0.3715, 0.0000, 0.0000, 1.2444, 0.0000, 2.4407, 0.4039],\n",
      "        [2.3238, 0.0190, 0.0825, 0.0000, 0.0000, 0.0043, 0.0598, 0.3208, 0.0742,\n",
      "         0.1626, 0.0000, 0.0000, 1.8155, 0.0000, 2.2069, 0.7861],\n",
      "        [1.3448, 0.0000, 1.1272, 0.0000, 0.0000, 0.1295, 2.0477, 0.0039, 1.3228,\n",
      "         2.4175, 0.0000, 0.0009, 0.1298, 1.1440, 2.4459, 0.0000],\n",
      "        [1.2433, 0.0000, 0.4410, 0.0000, 0.0000, 0.0266, 2.3677, 0.0000, 1.3651,\n",
      "         2.1602, 0.0000, 0.0000, 0.2409, 0.5024, 2.7963, 0.0258],\n",
      "        [0.0392, 0.0302, 0.0005, 0.0697, 0.5309, 0.0738, 0.0050, 0.0164, 6.2792,\n",
      "         0.1473, 1.2774, 4.8091, 1.1092, 0.0260, 0.2112, 5.2653],\n",
      "        [1.2771, 0.0000, 1.2814, 0.0000, 0.0131, 0.0160, 3.1829, 0.0402, 2.3968,\n",
      "         2.7700, 0.0037, 0.0009, 0.0812, 2.0396, 2.4394, 0.1422],\n",
      "        [0.0160, 0.1176, 0.1122, 2.1954, 1.6972, 1.9192, 0.1908, 1.8059, 0.0369,\n",
      "         1.2989, 0.6831, 2.0314, 0.0087, 2.0608, 0.0308, 0.0948]],\n",
      "       grad_fn=<StackBackward>)\n"
     ]
    }
   ],
   "source": [
    "c_means = find_cluster_means(out, label)\n",
    "print(c_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularization(cluster_means, norm=2):\n",
    "    reg = 0\n",
    "    n_clusters, feature_dim = cluster_means.shape\n",
    "    for i in range(n_clusters):\n",
    "        #print(torch.norm(cluster_means[i, :], norm))\n",
    "        reg += torch.norm(cluster_means[i, :], norm)\n",
    "    #print(reg)\n",
    "    reg /= n_clusters\n",
    "    return reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularization(c_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = (label == 0).squeeze(1).nonzero()\n",
    "index = index.squeeze(1)\n",
    "#print(out[index])\n",
    "#print(c_means[0])\n",
    "#print((out[index] - c_means[0]).shape)\n",
    "#print(torch.norm(out[index] - c_means[0], dim=1))\n",
    "dists = torch.norm(out[index] - c_means[0], dim=1)\n",
    "hinge = torch.clamp(dists - 1, 0)\n",
    "print(hinge.shape)\n",
    "l = torch.mean(torch.pow(hinge, 2))\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variance_loss(features, label, cluster_means, margin=1):\n",
    "    var_loss = 0\n",
    "    n_clusters = len(cluster_means)\n",
    "    cluster_labels = list(label.unique(sorted=True).numpy())\n",
    "    for i, c in enumerate(cluster_labels):\n",
    "        index = (label == c).squeeze(1).nonzero()\n",
    "        index = index.squeeze(1)\n",
    "        dists = torch.norm(features[index] - cluster_means[i], dim=1)\n",
    "        hinge = torch.clamp(dists-1, min=0)\n",
    "        l = torch.mean(torch.pow(hinge, 2))\n",
    "        var_loss += l\n",
    "    var_loss /= n_clusters\n",
    "    return var_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_loss(out, label, c_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Distance Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c1 = c_means[0]\n",
    "c2 = c_means[1]\n",
    "#print(\"c1 = {}, c2 = {}\".format(c1, c2))\n",
    "print(c1 - c2)\n",
    "print(torch.norm(c1 - c2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_distance_loss(cluster_means, margin=2):\n",
    "    mean_loss = 0\n",
    "    n_clusters = len(cluster_means)\n",
    "    for i, c1 in enumerate(cluster_means):\n",
    "        for j, c2 in enumerate(cluster_means):\n",
    "            if i != j:\n",
    "                dist = torch.norm(c1 - c2)\n",
    "                hinge = torch.clamp(2.0 * margin - dist, min=0)\n",
    "                mean_loss += torch.pow(hinge, 2)\n",
    "    mean_loss /= (n_clusters - 1) * n_clusters\n",
    "    return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_distance_loss(c_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules.loss import _Loss\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminativeLoss(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, delta_var=0.5, delta_dist=1.5, norm=2, \n",
    "                 alpha=1.0, beta=1.0, gamma=0.001,\n",
    "                 use_gpu=False):\n",
    "        super(DiscriminativeLoss, self).__init__()\n",
    "        self.delta_var = delta_var\n",
    "        self.delta_dist = delta_dist\n",
    "        self.norm = norm\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.use_gpu = use_gpu\n",
    "        \n",
    "    def find_cluster_means(self, features, label):\n",
    "        '''\n",
    "        For a given image, compute the mean clustering point mu_c for each\n",
    "        cluster label in the feature dimension.\n",
    "        '''\n",
    "        n_clusters = label.unique().size()\n",
    "        cluster_labels = list(label.unique(sorted=True).numpy())\n",
    "        # Ordering of the cluster means are crucial.\n",
    "        cluster_means = []\n",
    "        for c in cluster_labels:\n",
    "            index = (label == c).squeeze(1).nonzero()\n",
    "            index = index.squeeze(1)\n",
    "            mu_c = features[index].mean(0)\n",
    "            cluster_means.append(mu_c)\n",
    "        cluster_means = torch.stack(cluster_means)\n",
    "        print(cluster_means)\n",
    "        return cluster_means\n",
    "        \n",
    "    def variance_loss(self, features, label, cluster_means, margin=1):\n",
    "        var_loss = 0\n",
    "        n_clusters = len(cluster_means)\n",
    "        cluster_labels = list(label.unique(sorted=True).numpy())\n",
    "        for i, c in enumerate(cluster_labels):\n",
    "            index = (label == c).squeeze(1).nonzero()\n",
    "            index = index.squeeze(1)\n",
    "            dists = torch.norm(features[index] - cluster_means[i], dim=1)\n",
    "            hinge = torch.clamp(dists-1, min=0)\n",
    "            l = torch.mean(torch.pow(hinge, 2))\n",
    "            var_loss += l\n",
    "        var_loss /= n_clusters\n",
    "        print(var_loss)\n",
    "        return var_loss\n",
    "    \n",
    "    def mean_distance_loss(self, cluster_means, margin=2):\n",
    "        mean_loss = 0\n",
    "        n_clusters = len(cluster_means)\n",
    "        for i, c1 in enumerate(cluster_means):\n",
    "            for j, c2 in enumerate(cluster_means):\n",
    "                if i != j:\n",
    "                    dist = torch.norm(c1 - c2)\n",
    "                    hinge = torch.clamp(2.0 * margin - dist, min=0)\n",
    "                    mean_loss += torch.pow(hinge, 2)\n",
    "        if n_clusters > 1:\n",
    "            mean_loss /= (n_clusters - 1) * n_clusters\n",
    "        print(mean_loss)\n",
    "        return mean_loss\n",
    "    \n",
    "    def regularization(self, cluster_means, norm=2):\n",
    "        reg = 0\n",
    "        n_clusters, feature_dim = cluster_means.shape\n",
    "        for i in range(n_clusters):\n",
    "            #print(torch.norm(cluster_means[i, :], norm))\n",
    "            reg += torch.norm(cluster_means[i, :], norm)\n",
    "        #print(reg)\n",
    "        reg /= n_clusters\n",
    "        print(reg)\n",
    "        return reg\n",
    "    \n",
    "    def combine(self, features, label):\n",
    "        \n",
    "        c_means = self.find_cluster_means(features, label)\n",
    "        loss_dist = self.mean_distance_loss(c_means, margin=self.delta_dist)\n",
    "        loss_var = self.variance_loss(features, label, c_means, margin=self.delta_var)\n",
    "        loss_reg = self.regularization(c_means, norm=self.norm)\n",
    "        \n",
    "        loss = self.alpha * loss_var + self.beta * loss_dist + self.gamma * loss_reg\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        \n",
    "        return self.combine(x, y)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = DiscriminativeLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.4266, 2.6478, 1.8963, 0.0000, 0.2725, 0.0150, 0.0118, 0.4292, 0.4624,\n",
      "         0.0122, 1.6712, 0.0000, 2.5536, 0.0003, 2.2377, 2.9073],\n",
      "        [2.0479, 2.0550, 1.1468, 0.0000, 0.3504, 0.0278, 0.0098, 0.3071, 1.1611,\n",
      "         0.0341, 1.7235, 0.0450, 2.3116, 0.0007, 2.0758, 3.0324],\n",
      "        [1.9780, 0.0322, 1.4784, 0.0000, 0.0001, 0.0200, 2.4804, 0.0051, 1.6261,\n",
      "         1.8203, 0.0006, 0.0010, 0.3292, 0.8389, 3.1172, 0.0036],\n",
      "        [1.8207, 0.9258, 1.3938, 0.0000, 0.3216, 0.0939, 0.0515, 0.9003, 0.2954,\n",
      "         0.1475, 0.6243, 0.0000, 1.7631, 0.0030, 2.1722, 2.0616],\n",
      "        [2.3648, 0.0000, 0.1677, 0.0000, 0.0000, 0.0230, 0.2310, 0.1347, 0.2688,\n",
      "         0.4499, 0.0000, 0.0000, 1.0779, 0.0000, 2.6447, 0.0225],\n",
      "        [2.5145, 0.0274, 0.1270, 0.0000, 0.0000, 0.0255, 0.1967, 0.2036, 0.2963,\n",
      "         0.3715, 0.0000, 0.0000, 1.2444, 0.0000, 2.4407, 0.4039],\n",
      "        [2.3238, 0.0190, 0.0825, 0.0000, 0.0000, 0.0043, 0.0598, 0.3208, 0.0742,\n",
      "         0.1626, 0.0000, 0.0000, 1.8155, 0.0000, 2.2069, 0.7861],\n",
      "        [1.3448, 0.0000, 1.1272, 0.0000, 0.0000, 0.1295, 2.0477, 0.0039, 1.3228,\n",
      "         2.4175, 0.0000, 0.0009, 0.1298, 1.1440, 2.4459, 0.0000],\n",
      "        [1.2433, 0.0000, 0.4410, 0.0000, 0.0000, 0.0266, 2.3677, 0.0000, 1.3651,\n",
      "         2.1602, 0.0000, 0.0000, 0.2409, 0.5024, 2.7963, 0.0258],\n",
      "        [0.0392, 0.0302, 0.0005, 0.0697, 0.5309, 0.0738, 0.0050, 0.0164, 6.2792,\n",
      "         0.1473, 1.2774, 4.8091, 1.1092, 0.0260, 0.2112, 5.2653],\n",
      "        [1.2771, 0.0000, 1.2814, 0.0000, 0.0131, 0.0160, 3.1829, 0.0402, 2.3968,\n",
      "         2.7700, 0.0037, 0.0009, 0.0812, 2.0396, 2.4394, 0.1422],\n",
      "        [0.0160, 0.1176, 0.1122, 2.1954, 1.6972, 1.9192, 0.1908, 1.8059, 0.0369,\n",
      "         1.2989, 0.6831, 2.0314, 0.0087, 2.0608, 0.0308, 0.0948]],\n",
      "       grad_fn=<StackBackward>)\n",
      "tensor(0.4527, grad_fn=<DivBackward0>)\n",
      "tensor(1.4170, grad_fn=<DivBackward0>)\n",
      "tensor(5.2793, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.8750, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.forward(out, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean_distance_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-c4ccfb45e7ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmean_distance_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_means\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmargin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'mean_distance_loss' is not defined"
     ]
    }
   ],
   "source": [
    "mean_distance_loss(c_means, margin=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
