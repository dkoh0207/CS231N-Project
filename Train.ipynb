{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.14/04\n"
     ]
    }
   ],
   "source": [
    "# Import Dependencies\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "#%matplotlib notebook\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../new_notebooks/ipynb/dlp_opendata_api\")\n",
    "sys.path.append(\"../new_notebooks/ipynb\")\n",
    "from osf.image_api import image_reader_3d\n",
    "from osf.particle_api import *\n",
    "from osf.cluster_api import *\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import sparseconvnet as scn\n",
    "import glob\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "\n",
    "TESTSET_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ls /gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UResNet(torch.nn.Module):\n",
    "    def __init__(self, dim=3, size=192, nFeatures=16, depth=5, nClasses=5):\n",
    "        import sparseconvnet as scn\n",
    "        super(UResNet, self).__init__()\n",
    "        #self._flags = flags\n",
    "        dimension = dim\n",
    "        reps = 2  # Conv block repetition factor\n",
    "        kernel_size = 2  # Use input_spatial_size method for other values?\n",
    "        m = nFeatures  # Unet number of features\n",
    "        nPlanes = [i*m for i in range(1, depth+1)]  # UNet number of features per level\n",
    "        # nPlanes = [(2**i) * m for i in range(1, num_strides+1)]  # UNet number of features per level\n",
    "        nInputFeatures = 1\n",
    "        self.sparseModel = scn.Sequential().add(\n",
    "           scn.InputLayer(dimension, size, mode=3)).add(\n",
    "           scn.SubmanifoldConvolution(dimension, nInputFeatures, m, 3, False)).add( # Kernel size 3, no bias\n",
    "           scn.UNet(dimension, reps, nPlanes, residual_blocks=True, downsample=[kernel_size, 2])).add(  # downsample = [filter size, filter stride]\n",
    "           scn.BatchNormReLU(m)).add(\n",
    "           scn.OutputLayer(dimension))\n",
    "        self.linear = torch.nn.Linear(m, nClasses)\n",
    "\n",
    "    def forward(self, point_cloud):\n",
    "        \"\"\"\n",
    "        point_cloud is a list of length minibatch size (assumes mbs = 1)\n",
    "        point_cloud[0] has 3 spatial coordinates + 1 batch coordinate + 1 feature\n",
    "        shape of point_cloud[0] = (N, 4)\n",
    "        \"\"\"\n",
    "        #coords = point_cloud[:, 0:-1].float()\n",
    "        #features = point_cloud[:, -1][:, None].float()\n",
    "        x = self.sparseModel(point_cloud)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unet(fname, dimension=3, size=192, nFeatures=16, depth=5, nClasses=5):\n",
    "    model = UResNet(dim=dimension, size=size, nFeatures=nFeatures, depth=depth, nClasses=nClasses)\n",
    "    model = nn.DataParallel(model)\n",
    "    #print(model.state_dict().keys())\n",
    "    checkpoint = torch.load(fname, map_location='cpu')\n",
    "    #print()\n",
    "    #print(checkpoint['state_dict'].keys())\n",
    "    model.load_state_dict(checkpoint['state_dict'], strict=True)\n",
    "    # just return the pre-trained unet\n",
    "    return model.module.sparseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = '/gpfs/slac/staas/fs1/g/neutrino/.scn_paper/new/sparse_is192_uns5_uf16_bs64/weights3/snapshot-29999.ckpt'\n",
    "unet = get_unet(fname)\n",
    "unet = unet.cuda()\n",
    "unet = unet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusteringAEData(Dataset):\n",
    "    \"\"\"\n",
    "    A customized data loader for clustering.\n",
    "    \"\"\"\n",
    "    def __init__(self, root, numPixels=192, filenames=None):\n",
    "        \"\"\"\n",
    "        Initialize Clustering Dataset\n",
    "\n",
    "        Inputs:\n",
    "            - root: root directory of dataset\n",
    "            - preload: if preload dataset into memory.\n",
    "        \"\"\"\n",
    "        self.cluster_filenames = []\n",
    "        self.energy_filenames = []\n",
    "        self.root = root\n",
    "        self.numPixels = str(numPixels)\n",
    "        \n",
    "        if filenames:\n",
    "            self.energy_filenames = filenames[0]\n",
    "            self.cluster_filenames = filenames[1]\n",
    "            print(self.energy_filenames)\n",
    "\n",
    "        self.energy_filenames.sort()\n",
    "        self.cluster_filenames.sort()\n",
    "        self.cluster_reader = cluster_reader(*self.cluster_filenames)\n",
    "        self.energy_reader = image_reader_3d(*self.energy_filenames)\n",
    "        self.len = self.energy_reader.entry_count()\n",
    "        assert self.len == self.cluster_reader.entry_count()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get a sample from dataset.\n",
    "        \"\"\"\n",
    "        voxel, label = self.cluster_reader.get_image(index)\n",
    "        _, energy, _ = self.energy_reader.get_image(index)\n",
    "        voxel, label = torch.from_numpy(voxel), torch.from_numpy(label)\n",
    "        energy = torch.from_numpy(energy)\n",
    "        energy = torch.unsqueeze(energy, dim=1)\n",
    "        label = torch.unsqueeze(label, dim=1).type(torch.LongTensor)\n",
    "        voxel = voxel.cuda()\n",
    "        energy = energy.cuda()\n",
    "        with torch.no_grad():\n",
    "            out = unet((voxel, energy))\n",
    "        return out, label\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Total number of sampels in dataset.\n",
    "        \"\"\"\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ae_collate(batch):\n",
    "    \"\"\"\n",
    "    Custom collate_fn for Autoencoder.\n",
    "    \"\"\"\n",
    "    data = [item[0] for item in batch]\n",
    "    target = [item[1] for item in batch]\n",
    "    return [data, target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/cluster/dlprod_cluster_192px_00.root\n",
      "/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/dlprod_192px_00.root\n",
      "/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/cluster/dlprod_cluster_192px_01.root\n",
      "/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/dlprod_192px_01.root\n",
      "/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/cluster/dlprod_cluster_192px_02.root\n",
      "/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/dlprod_192px_02.root\n",
      "/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/cluster/dlprod_cluster_192px_03.root\n",
      "/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/dlprod_192px_03.root\n",
      "/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/cluster/dlprod_cluster_192px_04.root\n",
      "/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/dlprod_192px_04.root\n",
      "/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/cluster/dlprod_cluster_192px_05.root\n",
      "/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/dlprod_192px_05.root\n",
      "/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/cluster/dlprod_cluster_192px_06.root\n",
      "/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/dlprod_192px_06.root\n",
      "/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/cluster/dlprod_cluster_192px_07.root\n",
      "/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/dlprod_192px_07.root\n",
      "/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/cluster/dlprod_cluster_192px_08.root\n",
      "/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/dlprod_192px_08.root\n",
      "['/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/dlprod_192px_00.root', '/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/dlprod_192px_01.root', '/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/dlprod_192px_02.root', '/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/dlprod_192px_03.root', '/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/dlprod_192px_04.root', '/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/dlprod_192px_05.root', '/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/dlprod_192px_06.root', '/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/dlprod_192px_07.root']\n",
      "['/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10/dlprod_192px_08.root']\n",
      "Number of entries in training set: 80000\n",
      "Number of entries in validation set: 10000\n"
     ]
    }
   ],
   "source": [
    "root = '/gpfs/slac/staas/fs1/g/neutrino/kterao/data/dlprod_ppn_v10' #replace with your own path to root folder. \n",
    "trainset_cluster = [root + '/cluster/dlprod_cluster_192px_0{}.root'.format(i) for i in range(8)]\n",
    "devset_cluster = [root + '/cluster/dlprod_cluster_192px_0{}.root'.format(8)]\n",
    "#testset_cluster = [root + '/cluster/dlprod_cluster_192px_0{}.root'.format(9)]\n",
    "\n",
    "trainset_energy = [root + '/dlprod_192px_0{}.root'.format(i) for i in range(8)]\n",
    "devset_energy = [root + '/dlprod_192px_0{}.root'.format(8)]\n",
    "#testset_energy = [root + '/dlprod_192px_0{}.root'.format(9)]\n",
    "\n",
    "for i, f in enumerate(trainset_cluster):\n",
    "    print(f)\n",
    "    print(trainset_energy[i])\n",
    "    \n",
    "for i, f in enumerate(devset_cluster):\n",
    "    print(f)\n",
    "    print(devset_energy[i])\n",
    "    \n",
    "#for i, f in enumerate(testset_cluster):\n",
    "#    print(f)\n",
    "#    print(testset_energy[i])\n",
    "\n",
    "trainset = ClusteringAEData(root, 192, filenames=[trainset_energy, trainset_cluster])\n",
    "devset = ClusteringAEData(root, 192, filenames=[devset_energy, devset_cluster])\n",
    "#testset = ClusteringAEData(root, 192, filenames=[testset_energy, testset_cluster])\n",
    "print('Number of entries in training set: {}'.format(len(trainset)))\n",
    "print('Number of entries in validation set: {}'.format(len(devset)))\n",
    "#print('Number of entries in test set: {}'.format(len(testset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset, batch_size=1, shuffle=True, collate_fn=ae_collate, num_workers=0, pin_memory=False)\n",
    "#devloader = DataLoader(devset, batch_size=1, shuffle=True, collate_fn=ae_collate, num_workers=0, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry, labels = trainset[48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5180, 16])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entry.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5180, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loss import DiscriminativeLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = DiscriminativeLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7519, device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(entry, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusteringMLP(nn.Module):\n",
    "    def __init__(self, input_dim=16, nHidden1=32, nHidden2=16, nClasses=3):\n",
    "        super(ClusteringMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, nHidden1)\n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        self.fc2 = nn.Linear(nHidden1, nHidden2)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight)\n",
    "        self.fc3 = nn.Linear(nHidden2, nClasses)\n",
    "        nn.init.kaiming_normal_(self.fc3.weight)\n",
    "        \n",
    "        self.bn_1 = nn.BatchNorm1d(nHidden1)\n",
    "        self.bn_2 = nn.BatchNorm1d(nHidden2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.bn_1(self.fc1(x)))\n",
    "        x = F.leaky_relu(self.bn_2(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClusteringMLP()\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(entry)\n",
    "loss = criterion(out, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.6401, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainiter = iter(trainloader)\n",
    "x_batch, y_batch = trainiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4297, 16])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "f_train_loss = open('train_loss.csv', 'w')\n",
    "f_acc = open('train_acc.csv', 'w')\n",
    "trainlossWriter = csv.writer(f_train_loss, delimiter=',')\n",
    "trainaccWriter = csv.writer(f_acc, delimiter=',')\n",
    "\n",
    "f_dev_loss = open('dev_loss.csv', 'w')\n",
    "devlossWriter = csv.writer(f_dev_loss, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('#classifer parameters', 1219)\n"
     ]
    }
   ],
   "source": [
    "training_epochs=10\n",
    "#training_epoch=scn.checkpoint_restore(unet,exp_name,'unet',use_cuda)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "print('#classifer parameters', sum([x.nelement() for x in model.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(checkpoint_path, model, optimizer):\n",
    "    # state_dict: a Python dictionary object that:\n",
    "    # - for a model, maps each layer to its parameter tensor;\n",
    "    # - for an optimizer, contains info about the optimizer’s states and hyperparameters used.\n",
    "    state = {\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict()}\n",
    "    torch.save(state, checkpoint_path)\n",
    "    print('model saved to %s' % checkpoint_path)\n",
    "    \n",
    "def load_checkpoint(checkpoint_path, model, optimizer):\n",
    "    state = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(state['state_dict'])\n",
    "    optimizer.load_state_dict(state['optimizer'])\n",
    "    print('model loaded from %s' % checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = torch.utils.data.sampler.RandomSampler(devset, True, TESTSET_SIZE)\n",
    "devloader = DataLoader(devset, sampler=sampler, batch_size=16, collate_fn=ae_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, devloader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for k, batch in enumerate(devloader):\n",
    "            x_batch = batch[0]\n",
    "            y_batch = batch[1]\n",
    "            for j, data in enumerate(x_batch):\n",
    "                out = model(data)\n",
    "                loss = criterion(out, y_batch[j])\n",
    "                test_loss += loss\n",
    "    return test_loss.item() / float(TESTSET_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples = 1/80000, Loss = 3.50177431107\n",
      "6.8834375\n",
      "Examples = 2/80000, Loss = 0.00177151232492\n",
      "Examples = 3/80000, Loss = 4.65874767303\n",
      "Examples = 4/80000, Loss = 4.93723869324\n",
      "Examples = 5/80000, Loss = 6.28511714935\n",
      "Examples = 6/80000, Loss = 5.02825737\n",
      "Examples = 7/80000, Loss = 0.0210904404521\n",
      "Examples = 8/80000, Loss = 5.36056137085\n",
      "Examples = 9/80000, Loss = 5.4809384346\n",
      "Examples = 10/80000, Loss = 2.99925899506\n",
      "Examples = 11/80000, Loss = 4.93933963776\n",
      "Examples = 12/80000, Loss = 2.34561610222\n",
      "Examples = 13/80000, Loss = 6.68533849716\n",
      "Examples = 14/80000, Loss = 5.29439067841\n",
      "Examples = 15/80000, Loss = 4.82317972183\n",
      "Examples = 16/80000, Loss = 0.0301759261638\n",
      "Examples = 17/80000, Loss = 5.05984973907\n",
      "Examples = 18/80000, Loss = 5.90257072449\n",
      "Examples = 19/80000, Loss = 5.78770065308\n",
      "Examples = 20/80000, Loss = 4.49145460129\n",
      "Examples = 21/80000, Loss = 4.50738763809\n",
      "Examples = 22/80000, Loss = 3.48338150978\n",
      "Examples = 23/80000, Loss = 4.33403110504\n",
      "Examples = 24/80000, Loss = 2.6847217083\n",
      "Examples = 25/80000, Loss = 2.93249893188\n",
      "Examples = 26/80000, Loss = 4.83858108521\n",
      "Examples = 27/80000, Loss = 3.5833029747\n",
      "Examples = 28/80000, Loss = 3.80872750282\n",
      "Examples = 29/80000, Loss = 5.34647989273\n",
      "Examples = 30/80000, Loss = 2.9631588459\n",
      "Examples = 31/80000, Loss = 3.44101333618\n",
      "Examples = 32/80000, Loss = 5.207239151\n",
      "Examples = 33/80000, Loss = 4.5771355629\n",
      "Examples = 34/80000, Loss = 3.89357829094\n",
      "Examples = 35/80000, Loss = 4.58938074112\n",
      "Examples = 36/80000, Loss = 3.94966411591\n",
      "Examples = 37/80000, Loss = 2.64229035378\n",
      "Examples = 38/80000, Loss = 4.59408950806\n",
      "Examples = 39/80000, Loss = 3.59398031235\n",
      "Examples = 40/80000, Loss = 4.88304901123\n",
      "Examples = 41/80000, Loss = 3.75356626511\n",
      "Examples = 42/80000, Loss = 3.71762990952\n",
      "Examples = 43/80000, Loss = 4.32713079453\n",
      "Examples = 44/80000, Loss = 5.16422605515\n",
      "Examples = 45/80000, Loss = 4.43419599533\n",
      "Examples = 46/80000, Loss = 3.26905107498\n",
      "Examples = 47/80000, Loss = 5.1828212738\n",
      "Examples = 48/80000, Loss = 3.7739033699\n",
      "Examples = 49/80000, Loss = 2.33526539803\n",
      "Examples = 50/80000, Loss = 5.91445970535\n",
      "Examples = 51/80000, Loss = 3.73231053352\n",
      "Examples = 52/80000, Loss = 0.0567982494831\n",
      "Examples = 53/80000, Loss = 2.9638698101\n",
      "Examples = 54/80000, Loss = 5.21720409393\n",
      "Examples = 55/80000, Loss = 3.12668442726\n",
      "Examples = 56/80000, Loss = 4.99920511246\n",
      "Examples = 57/80000, Loss = 4.22047615051\n",
      "Examples = 58/80000, Loss = 5.23038959503\n",
      "Examples = 59/80000, Loss = 3.05718827248\n",
      "Examples = 60/80000, Loss = 4.57430505753\n",
      "Examples = 61/80000, Loss = 4.78143072128\n",
      "Examples = 62/80000, Loss = 4.43761491776\n",
      "Examples = 63/80000, Loss = 4.37341022491\n",
      "Examples = 64/80000, Loss = 3.44679236412\n",
      "Examples = 65/80000, Loss = 3.95296144485\n",
      "Examples = 66/80000, Loss = 2.35655546188\n",
      "Examples = 67/80000, Loss = 2.74997901917\n",
      "Examples = 68/80000, Loss = 2.90746855736\n",
      "Examples = 69/80000, Loss = 2.97435498238\n",
      "Examples = 70/80000, Loss = 1.35409832001\n",
      "Examples = 71/80000, Loss = 3.44866633415\n",
      "Examples = 72/80000, Loss = 4.03441429138\n",
      "Examples = 73/80000, Loss = 5.92496728897\n",
      "Examples = 74/80000, Loss = 3.69483137131\n",
      "Examples = 75/80000, Loss = 2.85333418846\n",
      "Examples = 76/80000, Loss = 2.63797187805\n",
      "Examples = 77/80000, Loss = 2.50831151009\n",
      "Examples = 78/80000, Loss = 2.36258101463\n",
      "Examples = 79/80000, Loss = 0.132494240999\n",
      "Examples = 80/80000, Loss = 2.63517141342\n",
      "Examples = 81/80000, Loss = 3.28623461723\n",
      "Examples = 82/80000, Loss = 2.12872552872\n",
      "Examples = 83/80000, Loss = 5.05979776382\n",
      "Examples = 84/80000, Loss = 1.39690899849\n",
      "Examples = 85/80000, Loss = 3.24288010597\n",
      "Examples = 86/80000, Loss = 1.91300952435\n",
      "Examples = 87/80000, Loss = 4.60675525665\n",
      "Examples = 88/80000, Loss = 0.118778660893\n",
      "Examples = 89/80000, Loss = 2.49503040314\n",
      "Examples = 90/80000, Loss = 2.26530098915\n",
      "Examples = 91/80000, Loss = 1.42577731609\n",
      "Examples = 92/80000, Loss = 1.25822293758\n",
      "Examples = 93/80000, Loss = 1.83194959164\n",
      "Examples = 94/80000, Loss = 2.93378663063\n",
      "Examples = 95/80000, Loss = 2.05239152908\n",
      "Examples = 96/80000, Loss = 4.72604227066\n",
      "Examples = 97/80000, Loss = 2.86491131783\n",
      "Examples = 98/80000, Loss = 2.13777542114\n",
      "Examples = 99/80000, Loss = 3.021941185\n",
      "Examples = 100/80000, Loss = 2.93249797821\n",
      "Examples = 101/80000, Loss = 3.33662128448\n",
      "3.26331756592\n",
      "Examples = 102/80000, Loss = 2.71626496315\n",
      "Examples = 103/80000, Loss = 2.18677711487\n",
      "Examples = 104/80000, Loss = 2.00293397903\n",
      "Examples = 105/80000, Loss = 2.2615749836\n",
      "Examples = 106/80000, Loss = 0.806975603104\n",
      "Examples = 107/80000, Loss = 2.72254943848\n",
      "Examples = 108/80000, Loss = 4.30480098724\n",
      "Examples = 109/80000, Loss = 5.74907302856\n",
      "Examples = 110/80000, Loss = 1.85624408722\n",
      "Examples = 111/80000, Loss = 1.5021045208\n",
      "Examples = 112/80000, Loss = 1.62234032154\n",
      "Examples = 113/80000, Loss = 2.60879945755\n",
      "Examples = 114/80000, Loss = 1.50876998901\n",
      "Examples = 115/80000, Loss = 1.62066960335\n",
      "Examples = 116/80000, Loss = 2.08748340607\n",
      "Examples = 117/80000, Loss = 2.1966176033\n",
      "Examples = 118/80000, Loss = 1.46045446396\n",
      "Examples = 119/80000, Loss = 2.89720630646\n",
      "Examples = 120/80000, Loss = 1.09496295452\n",
      "Examples = 121/80000, Loss = 0.0766430720687\n",
      "Examples = 122/80000, Loss = 2.05841517448\n",
      "Examples = 123/80000, Loss = 3.55944848061\n",
      "Examples = 124/80000, Loss = 0.0503298826516\n",
      "Examples = 125/80000, Loss = 1.67630147934\n",
      "Examples = 126/80000, Loss = 1.71580326557\n",
      "Examples = 127/80000, Loss = 1.70069479942\n",
      "Examples = 128/80000, Loss = 2.04860377312\n",
      "Examples = 129/80000, Loss = 1.85582220554\n",
      "Examples = 130/80000, Loss = 2.46070408821\n",
      "Examples = 131/80000, Loss = 1.36161613464\n",
      "Examples = 132/80000, Loss = 1.78250384331\n",
      "Examples = 133/80000, Loss = 3.27643680573\n",
      "Examples = 134/80000, Loss = 1.96542108059\n",
      "Examples = 135/80000, Loss = 0.726134240627\n",
      "Examples = 136/80000, Loss = 1.14399933815\n",
      "Examples = 137/80000, Loss = 1.87873673439\n",
      "Examples = 138/80000, Loss = 1.4283747673\n",
      "Examples = 139/80000, Loss = 1.81619894505\n",
      "Examples = 140/80000, Loss = 2.42549324036\n",
      "Examples = 141/80000, Loss = 1.71624791622\n",
      "Examples = 142/80000, Loss = 1.6047976017\n",
      "Examples = 143/80000, Loss = 2.40554857254\n",
      "Examples = 144/80000, Loss = 2.01056432724\n",
      "Examples = 145/80000, Loss = 2.15612363815\n",
      "Examples = 146/80000, Loss = 1.04503619671\n",
      "Examples = 147/80000, Loss = 1.07440245152\n",
      "Examples = 148/80000, Loss = 0.844474494457\n",
      "Examples = 149/80000, Loss = 0.20025986433\n",
      "Examples = 150/80000, Loss = 1.70311248302\n",
      "Examples = 151/80000, Loss = 4.57897186279\n",
      "Examples = 152/80000, Loss = 2.00799942017\n",
      "Examples = 153/80000, Loss = 0.503419876099\n",
      "Examples = 154/80000, Loss = 0.212271556258\n",
      "Examples = 155/80000, Loss = 2.37195467949\n",
      "Examples = 156/80000, Loss = 1.56894409657\n",
      "Examples = 157/80000, Loss = 7.97831916809\n",
      "Examples = 158/80000, Loss = 1.62711310387\n",
      "Examples = 159/80000, Loss = 6.99889373779\n",
      "Examples = 160/80000, Loss = 1.77397274971\n",
      "Examples = 161/80000, Loss = 1.88283228874\n",
      "Examples = 162/80000, Loss = 1.11194145679\n",
      "Examples = 163/80000, Loss = 0.648695468903\n",
      "Examples = 164/80000, Loss = 1.11761653423\n",
      "Examples = 165/80000, Loss = 2.67948770523\n",
      "Examples = 166/80000, Loss = 1.17614793777\n",
      "Examples = 167/80000, Loss = 1.85872745514\n",
      "Examples = 168/80000, Loss = 1.16224706173\n",
      "Examples = 169/80000, Loss = 2.30595397949\n",
      "Examples = 170/80000, Loss = 1.18580949306\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-777e21ef3cf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/utils/data/dataloader.pyc\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-77c5499e39c4>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mGet\u001b[0m \u001b[0ma\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \"\"\"\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mvoxel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menergy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menergy_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mvoxel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoxel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/afs/slac.stanford.edu/u/nu/koh0207/Projects/new_notebooks/ipynb/dlp_opendata_api/osf/cluster_api.pyc\u001b[0m in \u001b[0;36mget_image\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0mvector\u001b[0m \u001b[0mof\u001b[0m \u001b[0mlength\u001b[0m \u001b[0mN\u001b[0m \u001b[0mof\u001b[0m \u001b[0mcluster\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \"\"\"\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mvoxels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_sparse3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sparse3d_group'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# voxels, cluster labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvoxels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/afs/slac.stanford.edu/u/nu/koh0207/Projects/new_notebooks/ipynb/dlp_opendata_api/osf/analysis_apis.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, entry)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \"\"\"\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_chains\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetEntry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ptrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_branch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in <TTreeCache::FillBuffer>: Inconsistency: fCurrentClusterStart=1093 fEntryCurrent=2158 fNextClusterStart=2186 but fCurrentEntry should not be in between the two\n",
      "Error in <TTreeCache::FillBuffer>: Inconsistency: fCurrentClusterStart=4324 fEntryCurrent=5370 fNextClusterStart=5405 but fCurrentEntry should not be in between the two\n",
      "Error in <TTreeCache::FillBuffer>: Inconsistency: fCurrentClusterStart=0 fEntryCurrent=967 fNextClusterStart=988 but fCurrentEntry should not be in between the two\n",
      "Error in <TTreeCache::FillBuffer>: Inconsistency: fCurrentClusterStart=0 fEntryCurrent=4240 fNextClusterStart=4352 but fCurrentEntry should not be in between the two\n",
      "Error in <TTreeCache::FillBuffer>: Inconsistency: fCurrentClusterStart=0 fEntryCurrent=4292 fNextClusterStart=4406 but fCurrentEntry should not be in between the two\n",
      "Error in <TTreeCache::FillBuffer>: Inconsistency: fCurrentClusterStart=5405 fEntryCurrent=7553 fNextClusterStart=7567 but fCurrentEntry should not be in between the two\n",
      "Error in <TTreeCache::FillBuffer>: Inconsistency: fCurrentClusterStart=0 fEntryCurrent=2152 fNextClusterStart=2181 but fCurrentEntry should not be in between the two\n",
      "Error in <TTreeCache::FillBuffer>: Inconsistency: fCurrentClusterStart=0 fEntryCurrent=2173 fNextClusterStart=2206 but fCurrentEntry should not be in between the two\n",
      "Error in <TTreeCache::FillBuffer>: Inconsistency: fCurrentClusterStart=0 fEntryCurrent=2158 fNextClusterStart=2186 but fCurrentEntry should not be in between the two\n",
      "Error in <TTreeCache::FillBuffer>: Inconsistency: fCurrentClusterStart=0 fEntryCurrent=8664 fNextClusterStart=8716 but fCurrentEntry should not be in between the two\n",
      "Error in <TTreeCache::FillBuffer>: Inconsistency: fCurrentClusterStart=0 fEntryCurrent=4352 fNextClusterStart=6414 but fCurrentEntry should not be in between the two\n",
      "Error in <TTreeCache::FillBuffer>: Inconsistency: fCurrentClusterStart=5465 fEntryCurrent=7399 fNextClusterStart=7651 but fCurrentEntry should not be in between the two\n",
      "Error in <TTreeCache::FillBuffer>: Inconsistency: fCurrentClusterStart=0 fEntryCurrent=2146 fNextClusterStart=2162 but fCurrentEntry should not be in between the two\n",
      "Error in <TTreeCache::FillBuffer>: Inconsistency: fCurrentClusterStart=6486 fEntryCurrent=7399 fNextClusterStart=7567 but fCurrentEntry should not be in between the two\n",
      "Error in <TTreeCache::FillBuffer>: Inconsistency: fCurrentClusterStart=2141 fEntryCurrent=6414 fNextClusterStart=6423 but fCurrentEntry should not be in between the two\n",
      "Error in <TTreeCache::FillBuffer>: Inconsistency: fCurrentClusterStart=0 fEntryCurrent=4304 fNextClusterStart=4362 but fCurrentEntry should not be in between the two\n",
      "Error in <TTreeCache::FillBuffer>: Inconsistency: fCurrentClusterStart=0 fEntryCurrent=4346 fNextClusterStart=4412 but fCurrentEntry should not be in between the two\n",
      "Error in <TTreeCache::FillBuffer>: Inconsistency: fCurrentClusterStart=2194 fEntryCurrent=6459 fNextClusterStart=6582 but fCurrentEntry should not be in between the two\n",
      "Error in <TTreeCache::FillBuffer>: Inconsistency: fCurrentClusterStart=2224 fEntryCurrent=6537 fNextClusterStart=6672 but fCurrentEntry should not be in between the two\n",
      "Error in <TTreeCache::FillBuffer>: Inconsistency: fCurrentClusterStart=1960 fEntryCurrent=3896 fNextClusterStart=3920 but fCurrentEntry should not be in between the two\n",
      "Error in <TTreeCache::FillBuffer>: Inconsistency: fCurrentClusterStart=1079 fEntryCurrent=3219 fNextClusterStart=3237 but fCurrentEntry should not be in between the two\n",
      "Error in <TTreeCache::FillBuffer>: Inconsistency: fCurrentClusterStart=3896 fEntryCurrent=4855 fNextClusterStart=4870 but fCurrentEntry should not be in between the two\n",
      "Error in <TTreeCache::FillBuffer>: Inconsistency: fCurrentClusterStart=0 fEntryCurrent=1948 fNextClusterStart=1976 but fCurrentEntry should not be in between the two\n",
      "Error in <TTreeCache::FillBuffer>: Inconsistency: fCurrentClusterStart=0 fEntryCurrent=2146 fNextClusterStart=2186 but fCurrentEntry should not be in between the two\n",
      "Error in <TTreeCache::FillBuffer>: Inconsistency: fCurrentClusterStart=5465 fEntryCurrent=7553 fNextClusterStart=7651 but fCurrentEntry should not be in between the two\n",
      "Error in <TTreeCache::FillBuffer>: Inconsistency: fCurrentClusterStart=5285 fEntryCurrent=6558 fNextClusterStart=7399 but fCurrentEntry should not be in between the two\n",
      "Error in <TTreeCache::FillBuffer>: Inconsistency: fCurrentClusterStart=6685 fEntryCurrent=8550 fNextClusterStart=8595 but fCurrentEntry should not be in between the two\n",
      "Error in <TTreeCache::FillBuffer>: Inconsistency: fCurrentClusterStart=7434 fEntryCurrent=8456 fNextClusterStart=8496 but fCurrentEntry should not be in between the two\n",
      "Error in <TTreeCache::FillBuffer>: Inconsistency: fCurrentClusterStart=2181 fEntryCurrent=6414 fNextClusterStart=6543 but fCurrentEntry should not be in between the two\n",
      "Error in <TTreeCache::FillBuffer>: Inconsistency: fCurrentClusterStart=2206 fEntryCurrent=6498 fNextClusterStart=6618 but fCurrentEntry should not be in between the two\n",
      "Error in <TTreeCache::FillBuffer>: Inconsistency: fCurrentClusterStart=0 fEntryCurrent=4240 fNextClusterStart=4276 but fCurrentEntry should not be in between the two\n",
      "Error in <TTreeCache::FillBuffer>: Inconsistency: fCurrentClusterStart=0 fEntryCurrent=4292 fNextClusterStart=4332 but fCurrentEntry should not be in between the two\n",
      "Error in <TTreeCache::FillBuffer>: Inconsistency: fCurrentClusterStart=0 fEntryCurrent=4306 fNextClusterStart=4388 but fCurrentEntry should not be in between the two\n",
      "Error in <TTreeCache::FillBuffer>: Inconsistency: fCurrentClusterStart=0 fEntryCurrent=4358 fNextClusterStart=4448 but fCurrentEntry should not be in between the two\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "trainset_len = len(trainset)\n",
    "\n",
    "for epoch in range(1, training_epochs+1):\n",
    "    stats = {}\n",
    "    start = time.time()\n",
    "    train_loss=0\n",
    "    for i,batch in enumerate(trainloader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        data = batch[0][0]\n",
    "        label = batch[1][0]\n",
    "#        try:\n",
    "        out = model(data)\n",
    "        out = out.cpu()\n",
    "        loss = criterion(out, label)\n",
    "        train_loss+=loss.item()\n",
    "        loss.backward()\n",
    "        print(\"Examples = {}/{}, Loss = {}\".format(i+1, trainset_len, loss))\n",
    "        optimizer.step()\n",
    "        trainlossWriter.writerow([loss.item()])\n",
    "        if i % 100 == 0:\n",
    "            dev_loss = test(model, devloader)\n",
    "            print(dev_loss)\n",
    "#        except:\n",
    "#            print(\"Warning: Error Encounterd!!\")\n",
    "#            continue\n",
    "    save_checkpoint('checkpoint{}.ckpt'.format(epoch), model, optimizer)\n",
    "#scn.checkpoint_save(unet,exp_name,'unet',epoch, use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
